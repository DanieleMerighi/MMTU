
==========
== CUDA ==
==========

CUDA Version 12.2.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.66s/it]
Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/llms/hub/models--meta-llama--Llama-3.2-3B-Instruct/.no_exist/0cb88a4f764b7a12671c53f0838cd831a0843b95/custom_generate'
2025-10-13 09:41:49,779 - huggingface_hub.file_download - ERROR - Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/llms/hub/models--meta-llama--Llama-3.2-3B-Instruct/.no_exist/0cb88a4f764b7a12671c53f0838cd831a0843b95/custom_generate'
Device set to use cuda:0
Evaluating mmtu.jsonl ...
Querying Qwen2-1.5B-Instruct:   0%|                                         | 0/125 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:   1%|▎                                | 1/125 [00:05<11:52,  5.75s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:   2%|▌                                | 2/125 [00:08<07:50,  3.82s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:   2%|▊                                | 3/125 [00:16<12:15,  6.03s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:   3%|█                                | 4/125 [00:16<07:26,  3.69s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:   4%|█▎                               | 5/125 [00:18<06:07,  3.06s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:   5%|█▌                               | 6/125 [00:23<07:00,  3.53s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:   6%|█▊                               | 7/125 [00:25<06:14,  3.18s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:   6%|██                               | 8/125 [00:28<05:50,  3.00s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:   7%|██▍                              | 9/125 [00:30<04:56,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:   8%|██▌                             | 10/125 [00:31<04:25,  2.31s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:   9%|██▊                             | 11/125 [00:39<07:48,  4.11s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  10%|███                             | 12/125 [00:41<06:10,  3.28s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  10%|███▎                            | 13/125 [00:48<08:28,  4.54s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  11%|███▌                            | 14/125 [00:57<10:33,  5.71s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  12%|███▊                            | 15/125 [01:04<11:36,  6.34s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  13%|████                            | 16/125 [01:05<08:32,  4.71s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  14%|████▎                           | 17/125 [01:14<10:33,  5.87s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  14%|████▌                           | 18/125 [01:15<07:55,  4.44s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  15%|████▊                           | 19/125 [01:16<06:10,  3.49s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  16%|█████                           | 20/125 [01:24<08:31,  4.88s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  17%|█████▍                          | 21/125 [01:25<06:22,  3.67s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  18%|█████▋                          | 22/125 [01:26<04:58,  2.90s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  18%|█████▉                          | 23/125 [01:27<03:54,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  19%|██████▏                         | 24/125 [01:28<03:00,  1.78s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  20%|██████▍                         | 25/125 [01:28<02:21,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  21%|██████▋                         | 26/125 [01:35<04:43,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  22%|██████▉                         | 27/125 [01:40<05:59,  3.66s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  22%|███████▏                        | 28/125 [01:47<07:20,  4.54s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  23%|███████▍                        | 29/125 [01:53<08:06,  5.07s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  24%|███████▋                        | 30/125 [02:00<09:06,  5.76s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  25%|███████▉                        | 31/125 [02:08<09:56,  6.35s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  26%|████████▏                       | 32/125 [02:16<10:20,  6.67s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  26%|████████▍                       | 33/125 [02:23<10:33,  6.89s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  27%|████████▋                       | 34/125 [02:31<10:56,  7.22s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  28%|████████▉                       | 35/125 [02:39<10:59,  7.33s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  29%|█████████▏                      | 36/125 [02:39<07:58,  5.37s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  30%|█████████▍                      | 37/125 [02:40<05:51,  3.99s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  30%|█████████▋                      | 38/125 [02:41<04:22,  3.02s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  31%|█████████▉                      | 39/125 [02:42<03:20,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  32%|██████████▏                     | 40/125 [02:42<02:36,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  33%|██████████▍                     | 41/125 [02:43<02:09,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  34%|██████████▊                     | 42/125 [02:44<01:53,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  34%|███████████                     | 43/125 [02:45<01:36,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  35%|███████████▎                    | 44/125 [02:50<03:22,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  36%|███████████▌                    | 45/125 [02:51<02:40,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  37%|███████████▊                    | 46/125 [02:53<02:23,  1.81s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  38%|████████████                    | 47/125 [02:54<02:00,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  38%|████████████▎                   | 48/125 [02:55<01:58,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  39%|████████████▌                   | 49/125 [02:56<01:52,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  40%|████████████▊                   | 50/125 [02:57<01:34,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  41%|█████████████                   | 51/125 [03:06<04:10,  3.38s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  42%|█████████████▎                  | 52/125 [03:14<06:07,  5.03s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  42%|█████████████▌                  | 53/125 [03:23<07:17,  6.08s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  43%|█████████████▊                  | 54/125 [03:31<07:59,  6.75s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  44%|██████████████                  | 55/125 [03:33<06:12,  5.31s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  45%|██████████████▎                 | 56/125 [03:41<06:47,  5.91s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  46%|██████████████▌                 | 57/125 [03:48<07:09,  6.31s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  46%|██████████████▊                 | 58/125 [03:55<07:29,  6.71s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  47%|███████████████                 | 59/125 [04:03<07:36,  6.92s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  48%|███████████████▎                | 60/125 [04:10<07:38,  7.05s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  49%|███████████████▌                | 61/125 [04:13<06:06,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  50%|███████████████▊                | 62/125 [04:14<04:41,  4.47s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  50%|████████████████▏               | 63/125 [04:17<04:01,  3.90s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  51%|████████████████▍               | 64/125 [04:19<03:24,  3.34s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  52%|████████████████▋               | 65/125 [04:21<02:50,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  53%|████████████████▉               | 66/125 [04:29<04:17,  4.36s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  54%|█████████████████▏              | 67/125 [04:36<05:13,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  54%|█████████████████▍              | 68/125 [04:45<05:57,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  55%|█████████████████▋              | 69/125 [04:53<06:16,  6.73s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  56%|█████████████████▉              | 70/125 [05:00<06:28,  7.07s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  57%|██████████████████▏             | 71/125 [05:08<06:35,  7.33s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  58%|██████████████████▍             | 72/125 [05:17<06:47,  7.69s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  58%|██████████████████▋             | 73/125 [05:18<04:50,  5.59s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  60%|███████████████████▏            | 75/125 [05:28<04:26,  5.34s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  61%|███████████████████▍            | 76/125 [05:28<03:25,  4.20s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  62%|███████████████████▋            | 77/125 [05:29<02:38,  3.31s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  62%|███████████████████▉            | 78/125 [05:30<02:04,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  63%|████████████████████▏           | 79/125 [05:31<01:37,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  64%|████████████████████▍           | 80/125 [05:32<01:18,  1.75s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  65%|████████████████████▋           | 81/125 [05:33<01:09,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  66%|████████████████████▉           | 82/125 [05:33<00:51,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  66%|█████████████████████▏          | 83/125 [05:34<00:47,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  67%|█████████████████████▌          | 84/125 [05:35<00:43,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  68%|█████████████████████▊          | 85/125 [05:36<00:39,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  69%|██████████████████████          | 86/125 [05:37<00:44,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  70%|██████████████████████▎         | 87/125 [05:38<00:37,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  70%|██████████████████████▌         | 88/125 [05:42<01:15,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  71%|██████████████████████▊         | 89/125 [05:50<02:12,  3.68s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  72%|███████████████████████         | 90/125 [05:51<01:38,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  73%|███████████████████████▎        | 91/125 [05:51<01:12,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Failed query: CUDA out of memory. Tried to allocate 206.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 131.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.71 GiB memory in use. Of the allocated memory 3.88 GiB is allocated by PyTorch, and 537.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 131.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.71 GiB memory in use. Of the allocated memory 3.69 GiB is allocated by PyTorch, and 729.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 194.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 131.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.71 GiB memory in use. Of the allocated memory 3.79 GiB is allocated by PyTorch, and 634.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 178.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 131.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.71 GiB memory in use. Of the allocated memory 3.65 GiB is allocated by PyTorch, and 775.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 131.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.71 GiB memory in use. Of the allocated memory 3.65 GiB is allocated by PyTorch, and 776.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 396.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 39.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.80 GiB memory in use. Of the allocated memory 3.78 GiB is allocated by PyTorch, and 735.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 278.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 39.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.80 GiB memory in use. Of the allocated memory 3.44 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 311.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.54 GiB memory in use. Of the allocated memory 3.73 GiB is allocated by PyTorch, and 509.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 344.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 125.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.72 GiB memory in use. Of the allocated memory 3.83 GiB is allocated by PyTorch, and 600.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 326.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 125.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.72 GiB memory in use. Of the allocated memory 3.65 GiB is allocated by PyTorch, and 775.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 326.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 125.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.72 GiB memory in use. Of the allocated memory 3.65 GiB is allocated by PyTorch, and 775.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 330.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 125.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.72 GiB memory in use. Of the allocated memory 3.68 GiB is allocated by PyTorch, and 753.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 546.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 485.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.37 GiB memory in use. Of the allocated memory 3.48 GiB is allocated by PyTorch, and 589.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Querying Qwen2-1.5B-Instruct:  74%|███████████████████████▌        | 92/125 [05:52<00:54,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  74%|███████████████████████▊        | 93/125 [05:52<00:40,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  76%|████████████████████████▎       | 95/125 [05:52<00:22,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  77%|████████████████████████▌       | 96/125 [05:58<00:57,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  78%|████████████████████████▊       | 97/125 [06:05<01:34,  3.38s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  78%|█████████████████████████       | 98/125 [06:10<01:36,  3.56s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  79%|█████████████████████████▎      | 99/125 [06:16<01:51,  4.30s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  80%|████████████████████████▊      | 100/125 [06:23<02:09,  5.19s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  81%|█████████████████████████      | 101/125 [06:31<02:23,  5.98s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  82%|█████████████████████████▎     | 102/125 [06:39<02:32,  6.64s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  82%|█████████████████████████▌     | 103/125 [06:47<02:34,  7.02s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  83%|█████████████████████████▊     | 104/125 [06:55<02:30,  7.17s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  84%|██████████████████████████     | 105/125 [07:03<02:29,  7.50s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  85%|██████████████████████████▎    | 106/125 [07:04<01:43,  5.47s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  86%|██████████████████████████▌    | 107/125 [07:07<01:25,  4.73s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  86%|██████████████████████████▊    | 108/125 [07:07<00:56,  3.35s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  87%|███████████████████████████    | 109/125 [07:18<01:29,  5.59s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  88%|███████████████████████████▎   | 110/125 [07:26<01:34,  6.27s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  89%|███████████████████████████▌   | 111/125 [07:27<01:08,  4.89s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  90%|███████████████████████████▊   | 112/125 [07:29<00:53,  4.11s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  90%|████████████████████████████   | 113/125 [07:32<00:42,  3.51s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  91%|████████████████████████████▎  | 114/125 [07:33<00:32,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  92%|████████████████████████████▌  | 115/125 [07:39<00:36,  3.67s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  93%|████████████████████████████▊  | 116/125 [07:46<00:43,  4.83s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  94%|█████████████████████████████  | 117/125 [07:52<00:40,  5.02s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  94%|█████████████████████████████▎ | 118/125 [07:59<00:40,  5.74s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  95%|█████████████████████████████▌ | 119/125 [08:06<00:36,  6.16s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  96%|█████████████████████████████▊ | 120/125 [08:14<00:32,  6.59s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  97%|██████████████████████████████ | 121/125 [08:19<00:24,  6.07s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  98%|██████████████████████████████▎| 122/125 [08:20<00:13,  4.64s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  98%|██████████████████████████████▌| 123/125 [08:24<00:08,  4.41s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct:  99%|██████████████████████████████▊| 124/125 [08:27<00:04,  4.01s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Querying Qwen2-1.5B-Instruct: 100%|███████████████████████████████| 125/125 [08:29<00:00,  3.34s/it]Querying Qwen2-1.5B-Instruct: 100%|███████████████████████████████| 125/125 [08:29<00:00,  4.07s/it]
Failed query: CUDA out of memory. Tried to allocate 358.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 129.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.71 GiB memory in use. Of the allocated memory 3.62 GiB is allocated by PyTorch, and 802.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 378.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 213.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.63 GiB memory in use. Of the allocated memory 3.47 GiB is allocated by PyTorch, and 872.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 350.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 213.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.63 GiB memory in use. Of the allocated memory 3.38 GiB is allocated by PyTorch, and 966.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 213.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.63 GiB memory in use. Of the allocated memory 3.30 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 630.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 317.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.53 GiB memory in use. Of the allocated memory 3.70 GiB is allocated by PyTorch, and 537.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 153.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.69 GiB memory in use. Of the allocated memory 4.05 GiB is allocated by PyTorch, and 340.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Failed query: CUDA out of memory. Tried to allocate 240.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 153.50 MiB is free. Process 297776 has 1.21 GiB memory in use. Process 589277 has 17.52 GiB memory in use. Process 876401 has 4.69 GiB memory in use. Of the allocated memory 3.86 GiB is allocated by PyTorch, and 539.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
All threads completed

Time taken: 509.26 seconds
